{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Feature engineering is the process of transforming data into features which better represents the underlying problem, resulting in improvement in machine learning performance. It is process of creating new input variable from the variable data. Creating date time feature like weekday month, creating a new feature by taking mean or anyother statistical calculation of existing feature, converting text into numeric feature, creating polynomial transofrmation features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Feature selection is the process of reducing number of feature to those features which are most important. Feature selection reduces the computation power required for machine learning, it helps in getting rid of the noise in the data. The features with non predictive power reduces the performance of model. There are 3 methods for feature selection. Intrinsic, filter and wrappper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Wrapper method create many models with different subsets of input features and select those features that result in the best performing model according to a performance metric. Filter methods use statistical techniques to evaluate the relationship between each input variable and the target variable, and these scores are used as the basis to rank and choose those input variables that will be used in the model. Filter methods are information gain, chi square test,variance threshold,correlation coefficient. Wrapper method are sequential feature selection algorithms,recursive feature elimination etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\n",
    "i. Describe the overall feature selection process. ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Feature selection is the process of reducing the number of input variables when developing a predictive model. Feature selection reduces the computation power required for machine learning, it helps in getting rid of the noise in the data and improve model performance. Tree based model like Random forest, decision tree as well as algorithms such as penalized regression models like Lasso are used for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Text data needs to be transformed into numerical representation which is called vectorization. Steps for featire engineering, tokenize, count, normalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Cosine similarity indicated how 2 documents are similar to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67530325]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "cosine_similarity([[2, 3, 2, 0, 2, 3, 3, 0, 1]], [[2, 1, 0, 0, 3, 2, 1, 3, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Ans: Hamming distance is a metric for comparing two binary data strings.\n",
    "def hammingDist(str1, str2):\n",
    "    i = 0\n",
    "    count = 0\n",
    " \n",
    "    while(i < len(str1)):\n",
    "        if(str1[i] != str2[i]):\n",
    "            count += 1\n",
    "        i += 1\n",
    "    return count\n",
    " \n",
    "\n",
    "str1 = \"10001011\"\n",
    "str2 = \"11001111\"\n",
    "\n",
    "print(hammingDist(str1, str2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Jaccard index is used as a measure of similarity between two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "import numpy as np\n",
    "f1= np.array([[1, 1, 0, \n",
    "0, 1, 0, 1, 1]])\n",
    "f2= np.array([[1, 1, 0, 0, 0, 1, 1, 1]])\n",
    "4/(1+1+6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.333333333333333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SMC\n",
    "4+3/(3+1+1+4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_score(f1,f2, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. State what is meant by \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: High dimensional data set is data set with high number of features. Example of high dimensional data set is social networking sites, DNA analysis which can have up to 450,000 variables. High-dimensional data requires high amount of time and computational resources, degradation of model performance due to unnecessary noise. To over come this issue we should do feature selection to get rid of unwanted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Make a few quick notes on:\n",
    "\n",
    "1.PCA is an acronym for Personal Component Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: PCA is used to reduce features used in machine learning algorithm. It is used to create new set of feature from orignal features which are similar to each other. The new features created are distinct.\n",
    "\n",
    "i) Use of vectors\n",
    "\n",
    "ii) Embedded technique Embedded aproach is similar to wrapper approach, as it used and inductive algorithm to evaluate the generated feature subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Make a comparison between:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "Ans: Searching for valuable feature when starts with all feature set and keep removing features. This search strategy is sequential backward exclusion -Searching for valuable feature when starts with empty set and keep adding features. This search strategy is sequential forward selection\n",
    "\n",
    "1. Function selection methods: filter vs. wrapper\n",
    "\n",
    "Ans: In filter method learning algorithm is not used to select feature - In wrapper method learning algorithm is used to select feature.\n",
    "\n",
    "In filter method statistical test conducted on features are pearson correlation,information gain,fisher score, chi-square, anova - In wrapper method algorithms like Random forest, decision tree is used\n",
    "\n",
    "1. SMC vs. Jaccard coefficient\n",
    "Ans: SMC includes cases where noth the features have a value 0- Jaccard coefficient does not includes cases when both feature have value 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
